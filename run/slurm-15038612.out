Python 3.12.1
/u/gkerex/pyenv/torch/bin/python
Job starting on gpua055.delta.ncsa.illinois.edu
Using device: cuda
Loading data from data/camels_LH.hdf5...
Split sizes -> train: 800, val: 100, test: 100
Input dim: 1, Target dim: 6
Parameters: ['Omega_m', 'sigma_8', 'A_SN1', 'A_AGN1', 'A_SN2', 'A_AGN2']
Input stats: mean=2.6878628730773926, std=6.628475666046143
Output stats: min=[0.1002], max=[3.99446]

Initial model:
============================================================
Set-DSR Model Summary
============================================================
Number of summaries (K): 8
Total complexity: 48.00
------------------------------------------------------------
s_0: (identity_s(SUM((m * logM))) + SUM(abs((square(m) * (logM + m)))))
     complexity: 8.00, depth: 6
s_1: SUM(m)
     complexity: 1.00, depth: 2
s_2: MEAN(m)
     complexity: 1.00, depth: 2
s_3: (identity_s(square_s(-0.8273)) + ((0.6451 * identity_s(identity_s(MEAN(m)))) - MEAN(m)))
     complexity: 7.00, depth: 7
s_4: identity_s(((MEAN(logM) + (SUM(m) + (1.7716 + MEAN(logM)))) + (square_s(SUM(logM)) - square_s(square_s(MEAN(m))))))
     complexity: 13.50, depth: 7
s_5: SUM(identity((logM + (square(m) - square(m)))))
     complexity: 5.00, depth: 6
s_6: identity_s(((square_s(MEAN(logM)) * (square_s(SUM(logM)) * square_s(SUM(logM)))) + SUM(abs((m * m)))))
     complexity: 12.00, depth: 7
s_7: 1.7164
     complexity: 0.50, depth: 1
============================================================

Using advanced GP evolver (subtree_mutation=True, const_opt=True)

============================================================
Phase 1: Evolutionary Search (GP)
============================================================

[Gen 0] Advancing curriculum to level 1
[Gen    0] fitness=161964.5113, val_mse=1.6232e+05, val_r2=-4587672.7020, complexity=35.50
[Gen    5] fitness=5689.4430, val_mse=5.8884e+03, val_r2=-166423.2607, complexity=19.50
[Gen   10] fitness=0.2891, val_mse=7.1791e-02, val_r2=-1.0290, complexity=21.50
[Gen   15] fitness=0.2486, val_mse=4.6608e-02, val_r2=-0.3173, complexity=20.00
[Gen   20] fitness=0.2288, val_mse=4.6770e-02, val_r2=-0.3219, complexity=18.00
[Gen   25] fitness=0.1986, val_mse=4.6547e-02, val_r2=-0.3156, complexity=15.00

[Gen 30] Advancing curriculum to level 2
[Gen   30] fitness=0.1958, val_mse=4.9025e-02, val_r2=-0.3856, complexity=14.50
[Gen   35] fitness=0.1711, val_mse=4.8986e-02, val_r2=-0.3845, complexity=12.00
[Gen   40] fitness=0.1669, val_mse=5.0270e-02, val_r2=-0.4208, complexity=11.50
[Gen   45] fitness=0.1661, val_mse=4.4265e-02, val_r2=-0.2511, complexity=12.00

Best fitness: 0.1661

============================================================
Phase 2: MLP Head Fine-tuning
============================================================
[Epoch    0] train_loss=3.0898e-02, val_loss=2.7268e-02, val_r2=0.2293
[Epoch    5] train_loss=2.4412e-02, val_loss=2.3726e-02, val_r2=0.3294
[Epoch   10] train_loss=2.1262e-02, val_loss=2.1668e-02, val_r2=0.3876
[Epoch   15] train_loss=2.0358e-02, val_loss=2.1690e-02, val_r2=0.3870
[Epoch   20] train_loss=2.0540e-02, val_loss=2.1667e-02, val_r2=0.3876
[Epoch   25] train_loss=2.0088e-02, val_loss=2.1434e-02, val_r2=0.3942
[Epoch   30] train_loss=2.0100e-02, val_loss=2.1484e-02, val_r2=0.3928
[Epoch   35] train_loss=1.9913e-02, val_loss=2.1587e-02, val_r2=0.3899
[Epoch   40] train_loss=1.9925e-02, val_loss=2.1642e-02, val_r2=0.3883
[Epoch   45] train_loss=1.9865e-02, val_loss=2.1479e-02, val_r2=0.3929

Best validation loss: 2.1238e-02

Training completed in 116.8s

Final model:
============================================================
Set-DSR Model Summary
============================================================
Number of summaries (K): 8
Total complexity: 12.00
------------------------------------------------------------
s_0: identity_s(identity_s(SUM(identity((m - m)))))
     complexity: 2.00, depth: 6
s_1: identity_s(identity_s(MEAN(logM)))
     complexity: 1.00, depth: 4
s_2: MEAN((logM - m))
     complexity: 2.00, depth: 3
s_3: identity_s(identity_s(0.5621))
     complexity: 0.50, depth: 3
s_4: SUM(identity(abs((identity(logM) - (logM * m)))))
     complexity: 4.00, depth: 6
s_5: 9.9984
     complexity: 0.50, depth: 1
s_6: MEAN(logM)
     complexity: 1.00, depth: 2
s_7: MEAN(m)
     complexity: 1.00, depth: 2
============================================================

Final Train MSE: 3.0014e-01, R2: 0.4508
Final Val MSE: 3.2541e-01, R2: 0.3935
Final Test MSE: 3.2411e-01, R2: 0.4044
Saved test plot: run/data/models/plots/set_dsr/pred_vs_true_test_final.png
Saved plots: run/data/models/plots/set_dsr/pred_vs_true_train_final.png, run/data/models/plots/set_dsr/pred_vs_true_val_final.png

Applying SymPy simplification...
Warning: SymPy simplification failed: 'SetDSR' object has no attribute 'items'
Saved expressions: run/data/models/set_dsr/expressions_20251231_162657.json
Saved checkpoint: run/data/models/set_dsr/set_dsr_20251231_162657.pt
Saved history: run/data/models/set_dsr/history_20251231_162657.json

============================================================
Learned Symbolic Summary Statistics:
============================================================
  s_0: identity_s(identity_s(SUM(identity((m - m)))))
  s_1: identity_s(identity_s(MEAN(logM)))
  s_2: MEAN((logM - m))
  s_3: identity_s(identity_s(0.5621))
  s_4: SUM(identity(abs((identity(logM) - (logM * m)))))
  s_5: 9.9984
  s_6: MEAN(logM)
  s_7: MEAN(m)
============================================================
Job completed successfully
