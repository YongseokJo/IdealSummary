Python 3.12.1
/u/gkerex/pyenv/torch/bin/python
Job starting on gpua053.delta.ncsa.illinois.edu
Using device: cuda
Loading data from data/camels_LH.hdf5...
Split sizes -> train: 800, val: 100, test: 100
Input dim: 1, Target dim: 6
Parameters: ['Omega_m', 'sigma_8', 'A_SN1', 'A_AGN1', 'A_SN2', 'A_AGN2']
Input stats: mean=2.6878628730773926, std=6.628475666046143
Output stats: min=[0.1002], max=[3.99446]

Initial model:
============================================================
Set-DSR Model Summary
============================================================
Number of summaries (K): 12
Total complexity: 121.00
------------------------------------------------------------
s_0: (identity_s(SUM((m * logM))) + SUM(abs((square(identity(logM)) * (logM - (logM - logM))))))
     complexity: 9.00, depth: 7
s_1: MEAN((logM * (m * m)))
     complexity: 3.00, depth: 4
s_2: square_s((0.6451 * identity_s(identity_s(MEAN((logM - m))))))
     complexity: 4.50, depth: 7
s_3: MEAN(((((square(logM) + (logM + logM)) * (logM - (m * logM))) * ((square(logM) * m) - ((m - logM) - (m + logM)))) * (logM + (square(square(m)) - ((m + m) + m)))))
     complexity: 21.00, depth: 7
s_4: SUM((logM * identity((((m + logM) * abs(logM)) - square(m)))))
     complexity: 7.00, depth: 7
s_5: square_s(SUM((m + (((logM * m) + abs(m)) * ((logM * m) * abs(m))))))
     complexity: 10.00, depth: 7
s_6: ((MEAN(logM) + (0.5244 + SUM((square(m) + logM)))) * square_s(MEAN((m + square(abs(logM))))))
     complexity: 12.50, depth: 7
s_7: (-0.3895 * SUM(logM))
     complexity: 2.50, depth: 3
s_8: ((((0.6086 * (identity_s(-0.0574) + MEAN(logM))) + SUM(((m - m) - (m * logM)))) - SUM((((logM + m) + abs(logM)) * logM))) + identity_s((square_s(SUM(identity(logM))) * ((identity_s(MEAN(logM)) - SUM(m)) * (0.4846 + (-1.1474 + 1.7330))))))
     complexity: 26.50, depth: 8
s_9: identity_s(SUM(abs(((abs(m) * (logM + m)) * logM))))
     complexity: 6.00, depth: 7
s_10: identity_s(SUM(((square((logM + m)) - (logM + (logM - m))) + square(identity(logM)))))
     complexity: 8.00, depth: 7
s_11: (((SUM(((logM * m) + (logM - logM))) + identity_s(identity_s(MEAN(m)))) + identity_s(SUM(identity((logM + m))))) - SUM(logM))
     complexity: 11.00, depth: 7
============================================================

Using advanced GP evolver (subtree_mutation=True, const_opt=True)

============================================================
Phase 1: Evolutionary Search (GP)
============================================================

[Gen 0] Advancing curriculum to level 1
[Gen    0] fitness=2195808512.2725, val_mse=2.2198e+09, val_r2=-62737144213.9411, complexity=54.50
[Gen   10] fitness=0.3844, val_mse=7.9095e-02, val_r2=-1.2355, complexity=61.00
[Gen   20] fitness=0.2548, val_mse=7.2064e-02, val_r2=-1.0368, complexity=36.50

[Gen 30] Advancing curriculum to level 2
[Gen   30] fitness=0.2031, val_mse=6.4859e-02, val_r2=-0.8331, complexity=27.50
[Gen   40] fitness=0.1976, val_mse=4.9449e-02, val_r2=-0.3976, complexity=29.50
[Gen   50] fitness=0.1669, val_mse=4.9288e-02, val_r2=-0.3930, complexity=23.50

[Gen 60] Advancing curriculum to level 3
[Gen   60] fitness=0.1403, val_mse=4.2713e-02, val_r2=-0.2072, complexity=19.50
[Gen   70] fitness=0.1336, val_mse=4.1100e-02, val_r2=-0.1616, complexity=18.50
[Gen   80] fitness=0.1327, val_mse=4.0321e-02, val_r2=-0.1396, complexity=18.50
[Gen   90] fitness=0.1247, val_mse=4.2559e-02, val_r2=-0.2028, complexity=16.50
[Gen  100] fitness=0.1246, val_mse=4.2425e-02, val_r2=-0.1991, complexity=16.50
[Gen  110] fitness=0.1244, val_mse=4.2297e-02, val_r2=-0.1954, complexity=16.50
[Gen  120] fitness=0.1243, val_mse=4.2186e-02, val_r2=-0.1923, complexity=16.50
[Gen  130] fitness=0.1242, val_mse=4.2064e-02, val_r2=-0.1889, complexity=16.50
[Gen  140] fitness=0.1227, val_mse=3.7531e-02, val_r2=-0.0608, complexity=17.00
[Gen  150] fitness=0.1225, val_mse=3.7406e-02, val_r2=-0.0572, complexity=17.00
[Gen  160] fitness=0.1216, val_mse=4.1643e-02, val_r2=-0.1770, complexity=16.00
[Gen  170] fitness=0.1214, val_mse=4.1501e-02, val_r2=-0.1730, complexity=16.00
[Gen  180] fitness=0.1212, val_mse=4.1305e-02, val_r2=-0.1674, complexity=16.00
[Gen  190] fitness=0.1162, val_mse=4.1335e-02, val_r2=-0.1683, complexity=15.00
[Gen  200] fitness=0.1158, val_mse=4.0938e-02, val_r2=-0.1570, complexity=15.00
[Gen  210] fitness=0.1156, val_mse=4.0742e-02, val_r2=-0.1515, complexity=15.00
[Gen  220] fitness=0.1155, val_mse=4.0626e-02, val_r2=-0.1482, complexity=15.00
[Gen  230] fitness=0.1145, val_mse=3.9401e-02, val_r2=-0.1136, complexity=15.00
[Gen  240] fitness=0.1144, val_mse=3.9350e-02, val_r2=-0.1122, complexity=15.00
[Gen  250] fitness=0.1143, val_mse=3.9301e-02, val_r2=-0.1108, complexity=15.00
[Gen  260] fitness=0.1113, val_mse=3.6134e-02, val_r2=-0.0212, complexity=15.00
[Gen  270] fitness=0.1111, val_mse=3.5987e-02, val_r2=-0.0171, complexity=15.00
[Gen  280] fitness=0.1110, val_mse=3.5894e-02, val_r2=-0.0145, complexity=15.00
[Gen  290] fitness=0.1109, val_mse=3.5830e-02, val_r2=-0.0127, complexity=15.00

Best fitness: 0.1109

============================================================
Phase 2: MLP Head Fine-tuning
============================================================
[Epoch    0] train_loss=2.4374e-02, val_loss=2.0804e-02, val_r2=0.4120
[Epoch   10] train_loss=1.8733e-02, val_loss=2.0729e-02, val_r2=0.4141
[Epoch   20] train_loss=1.8502e-02, val_loss=2.0415e-02, val_r2=0.4230
[Epoch   30] train_loss=1.8451e-02, val_loss=2.0116e-02, val_r2=0.4315
[Epoch   40] train_loss=1.8281e-02, val_loss=2.0058e-02, val_r2=0.4331
[Epoch   50] train_loss=1.7875e-02, val_loss=1.9894e-02, val_r2=0.4377
[Epoch   60] train_loss=1.7794e-02, val_loss=2.0183e-02, val_r2=0.4296
[Epoch   70] train_loss=1.7465e-02, val_loss=1.9003e-02, val_r2=0.4629
[Epoch   80] train_loss=1.7458e-02, val_loss=1.9119e-02, val_r2=0.4596
[Epoch   90] train_loss=1.7599e-02, val_loss=1.8796e-02, val_r2=0.4688
[Epoch  100] train_loss=1.7254e-02, val_loss=1.8707e-02, val_r2=0.4713
[Epoch  110] train_loss=1.7312e-02, val_loss=1.8693e-02, val_r2=0.4717
[Epoch  120] train_loss=1.7191e-02, val_loss=1.8812e-02, val_r2=0.4683
[Epoch  130] train_loss=1.7036e-02, val_loss=1.8490e-02, val_r2=0.4774
[Epoch  140] train_loss=1.7045e-02, val_loss=1.8499e-02, val_r2=0.4772
[Epoch  150] train_loss=1.7025e-02, val_loss=1.8540e-02, val_r2=0.4760
[Epoch  160] train_loss=1.7008e-02, val_loss=1.8546e-02, val_r2=0.4758
[Epoch  170] train_loss=1.7001e-02, val_loss=1.8511e-02, val_r2=0.4768
[Epoch  180] train_loss=1.6992e-02, val_loss=1.8507e-02, val_r2=0.4769
[Epoch  190] train_loss=1.6989e-02, val_loss=1.8508e-02, val_r2=0.4769

Best validation loss: 1.8480e-02

Training completed in 1438.6s

Final model:
============================================================
Set-DSR Model Summary
============================================================
Number of summaries (K): 12
Total complexity: 15.00
------------------------------------------------------------
s_0: MEAN(identity(square(square(logM))))
     complexity: 3.00, depth: 5
s_1: identity_s(-2.0477)
     complexity: 0.50, depth: 2
s_2: SUM((logM * (logM - identity(logM))))
     complexity: 3.00, depth: 5
s_3: MEAN(logM)
     complexity: 1.00, depth: 2
s_4: identity_s(identity_s(1.8833))
     complexity: 0.50, depth: 3
s_5: MEAN(identity(logM))
     complexity: 1.00, depth: 3
s_6: square_s(1.9448)
     complexity: 1.50, depth: 2
s_7: identity_s(0.2405)
     complexity: 0.50, depth: 2
s_8: MEAN(logM)
     complexity: 1.00, depth: 2
s_9: MEAN(logM)
     complexity: 1.00, depth: 2
s_10: MEAN(m)
     complexity: 1.00, depth: 2
s_11: MEAN(m)
     complexity: 1.00, depth: 2
============================================================

Final Train MSE: 2.5763e-01, R2: 0.5286
Final Val MSE: 2.8073e-01, R2: 0.4768
Final Test MSE: 2.8886e-01, R2: 0.4691
Saved test plot: run/data/models/plots/set_dsr/pred_vs_true_test_final.png
Saved plots: run/data/models/plots/set_dsr/pred_vs_true_train_final.png, run/data/models/plots/set_dsr/pred_vs_true_val_final.png

Applying SymPy simplification...
Warning: SymPy simplification failed: 'SetDSR' object has no attribute 'items'
Saved expressions: run/data/models/set_dsr/expressions_20251231_170959.json
Saved checkpoint: run/data/models/set_dsr/set_dsr_20251231_170959.pt
Saved history: run/data/models/set_dsr/history_20251231_170959.json

============================================================
Learned Symbolic Summary Statistics:
============================================================
  s_0: MEAN(identity(square(square(logM))))
  s_1: identity_s(-2.0477)
  s_2: SUM((logM * (logM - identity(logM))))
  s_3: MEAN(logM)
  s_4: identity_s(identity_s(1.8833))
  s_5: MEAN(identity(logM))
  s_6: square_s(1.9448)
  s_7: identity_s(0.2405)
  s_8: MEAN(logM)
  s_9: MEAN(logM)
  s_10: MEAN(m)
  s_11: MEAN(m)
============================================================
Job completed successfully
