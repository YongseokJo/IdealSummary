Running "module reset". Resetting modules to system default. The following $MODULEPATH directories have been removed: /sw/rh9.4/spack/default/modules/lmod/linux-rhel9-x86_64/Core
Lmod has detected the following error: The following module(s) are unknown:
"cuda/11.8.0"

Please check the spelling or version number. Also try "module spider ..."
It is also possible your cache file is out-of-date; it may help to try:
  $ module --ignore_cache load "cuda/11.8.0"

Also make sure that all modulefiles written in TCL start with the string
#%Module




Currently Loaded Modules:
  1) gcc-native/13.2       9) craype-x86-milan
  2) craype/2.7.34        10) cudatoolkit/25.3_12.8
  3) libfabric/1.22.0     11) craype-accel-nvidia80
  4) craype-network-ofi   12) cue-login-env/1.1
  5) cray-mpich/8.1.32    13) slurm-env/0.1
  6) cray-libsci/25.03.0  14) default
  7) PrgEnv-gnu/8.6.0     15) python/3.13.5-gcc13.3.1
  8) cray-dsmml/0.3.1     16) ffmpeg/7.1

 

+ export OMP_NUM_THREADS=6
+ OMP_NUM_THREADS=6
+ export MKL_NUM_THREADS=6
+ MKL_NUM_THREADS=6
+ '[' -d /u/gkerex/pyenv/torch ']'
+ source /u/gkerex/pyenv/torch/bin/activate
++ deactivate nondestructive
++ '[' -n '' ']'
++ '[' -n '' ']'
++ hash -r
++ '[' -n '' ']'
++ unset VIRTUAL_ENV
++ unset VIRTUAL_ENV_PROMPT
++ '[' '!' nondestructive = nondestructive ']'
++ '[' linux-gnu = cygwin ']'
++ '[' linux-gnu = msys ']'
++ export VIRTUAL_ENV=/u/gkerex/pyenv/torch
++ VIRTUAL_ENV=/u/gkerex/pyenv/torch
++ _OLD_VIRTUAL_PATH=/sw/rh9.4/spack/v1.0.0/sw/linux-x86_64_v2/ffmpeg-7.1-3avnbo4/bin:/sw/rh9.4/spack/v1.0.0/sw/linux-x86_64_v2/python-3.13.5-7bp2p4z/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/25.3/cuda/12.8/compute-sanitizer:/opt/nvidia/hpc_sdk/Linux_x86_64/25.3/cuda/12.8/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/25.3/cuda/12.8/libnvvp:/opt/nvidia/hpc_sdk/Linux_x86_64/25.3/profilers/Nsight_Compute:/opt/nvidia/hpc_sdk/Linux_x86_64/25.3/profilers/Nsight_Systems/bin:/opt/cray/pe/mpich/8.1.32/ofi/gnu/11.2/bin:/opt/cray/pe/mpich/8.1.32/bin:/opt/cray/libfabric/1.22.0/bin:/opt/cray/pe/craype/2.7.34/bin:/opt/rh/gcc-toolset-13/root/usr/bin:/sw/rh9.4/user/scripts:/sw/user/scripts:/u/gkerex/.vscode-server/data/User/globalStorage/github.copilot-chat/debugCommand:/u/gkerex/.vscode-server/data/User/globalStorage/github.copilot-chat/copilotCli:/u/gkerex/.vscode-server/cli/servers/Stable-994fd12f8d3a5aa16f17d42c041e5809167e845a/server/bin/remote-cli:/u/gkerex/pkg/hpc-setup-kit/rclone:/u/gkerex/.local/bin:/u/gkerex/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/puppetlabs/bin:/u/gkerex/.vscode-server/extensions/ms-python.debugpy-2025.18.0-linux-x64/bundled/scripts/noConfigScripts:/opt/cray/pe/bin
++ PATH=/u/gkerex/pyenv/torch/bin:/sw/rh9.4/spack/v1.0.0/sw/linux-x86_64_v2/ffmpeg-7.1-3avnbo4/bin:/sw/rh9.4/spack/v1.0.0/sw/linux-x86_64_v2/python-3.13.5-7bp2p4z/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/25.3/cuda/12.8/compute-sanitizer:/opt/nvidia/hpc_sdk/Linux_x86_64/25.3/cuda/12.8/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/25.3/cuda/12.8/libnvvp:/opt/nvidia/hpc_sdk/Linux_x86_64/25.3/profilers/Nsight_Compute:/opt/nvidia/hpc_sdk/Linux_x86_64/25.3/profilers/Nsight_Systems/bin:/opt/cray/pe/mpich/8.1.32/ofi/gnu/11.2/bin:/opt/cray/pe/mpich/8.1.32/bin:/opt/cray/libfabric/1.22.0/bin:/opt/cray/pe/craype/2.7.34/bin:/opt/rh/gcc-toolset-13/root/usr/bin:/sw/rh9.4/user/scripts:/sw/user/scripts:/u/gkerex/.vscode-server/data/User/globalStorage/github.copilot-chat/debugCommand:/u/gkerex/.vscode-server/data/User/globalStorage/github.copilot-chat/copilotCli:/u/gkerex/.vscode-server/cli/servers/Stable-994fd12f8d3a5aa16f17d42c041e5809167e845a/server/bin/remote-cli:/u/gkerex/pkg/hpc-setup-kit/rclone:/u/gkerex/.local/bin:/u/gkerex/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/puppetlabs/bin:/u/gkerex/.vscode-server/extensions/ms-python.debugpy-2025.18.0-linux-x64/bundled/scripts/noConfigScripts:/opt/cray/pe/bin
++ export PATH
++ '[' -n '' ']'
++ '[' -z '' ']'
++ _OLD_VIRTUAL_PS1=
++ PS1='(torch) '
++ export PS1
++ VIRTUAL_ENV_PROMPT='(torch) '
++ export VIRTUAL_ENV_PROMPT
++ hash -r
+ python -V
+ which python
+ alias
+ eval declare -f
++ declare -f
+ /usr/bin/which --tty-only --read-alias --read-functions --show-tilde --show-dot python
++ hostname
+ echo 'job is starting on gpua053.delta.ncsa.illinois.edu'
+ MODEL_TYPE=mlp
+ echo 'Selected MODEL_TYPE=mlp'
+ USE_PARAM_KEYS=1
+ PARAM_GROUP=3
+ GROUP_LABEL=
+ PARAM_KEYS=()
+ '[' -n '' ']'
+ case "$PARAM_GROUP" in
+ PARAM_KEYS=(14 15 16 17 18 19 20)
+ GROUP_LABEL=3
+ '[' 1 = 0 ']'
+ '[' 1 = false ']'
+ '[' 1 = no ']'
+ WAND_PROJECT_BASE=SB28_7params
+ WAND_PROJECT=SB28_7params_3
+ WAND_RUN_NAME=mlp
+ JOB_NAME=IS_mlp_3
+ echo JOB_NAME=IS_mlp_3
+ '[' -n 15065005 ']'
+ command -v scontrol
+ scontrol update JobId=15065005 JobName=IS_mlp_3
+ COMMON_ARGS=(--use-hdf5 --h5-path ../data/camels_SB28.hdf5 --snap 90 --train-frac 0.8 --val-frac 0.1 --test-frac 0.1 --normalize-input log --normalize-output minmax --save-model --epochs 5000 --lr=1e-3 --wandb --wandb-project "${WAND_PROJECT}" --wandb-run-name "${WAND_RUN_NAME}" --save-model --wandb-save-model)
+ case "$MODEL_TYPE" in
+ MODEL_ARGS=(--model-type mlp --use-smf)
+ '[' 1 = 0 ']'
+ '[' 1 = false ']'
+ '[' 1 = no ']'
+ PY_CMD=(python ../src/train.py "${COMMON_ARGS[@]}" "${MODEL_ARGS[@]}" --param-keys)
+ for k in "${PARAM_KEYS[@]}"
+ PY_CMD+=("$k")
+ for k in "${PARAM_KEYS[@]}"
+ PY_CMD+=("$k")
+ for k in "${PARAM_KEYS[@]}"
+ PY_CMD+=("$k")
+ for k in "${PARAM_KEYS[@]}"
+ PY_CMD+=("$k")
+ for k in "${PARAM_KEYS[@]}"
+ PY_CMD+=("$k")
+ for k in "${PARAM_KEYS[@]}"
+ PY_CMD+=("$k")
+ for k in "${PARAM_KEYS[@]}"
+ PY_CMD+=("$k")
+ echo 'Running: python ../src/train.py --use-hdf5 --h5-path ../data/camels_SB28.hdf5 --snap 90 --train-frac 0.8 --val-frac 0.1 --test-frac 0.1 --normalize-input log --normalize-output minmax --save-model --epochs 5000 --lr=1e-3 --wandb --wandb-project SB28_7params_3 --wandb-run-name mlp --save-model --wandb-save-model --model-type mlp --use-smf --param-keys 14 15 16 17 18 19 20'
+ python ../src/train.py --use-hdf5 --h5-path ../data/camels_SB28.hdf5 --snap 90 --train-frac 0.8 --val-frac 0.1 --test-frac 0.1 --normalize-input log --normalize-output minmax --save-model --epochs 5000 --lr=1e-3 --wandb --wandb-project SB28_7params_3 --wandb-run-name mlp --save-model --wandb-save-model --model-type mlp --use-smf --param-keys 14 15 16 17 18 19 20
wandb: Currently logged in as: kerex (kerex-northwestern-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run rnmuzwzb
wandb: Tracking run with wandb version 0.23.1
wandb: Run data is saved locally in /u/gkerex/projects/IdealSummary/run/wandb/run-20251231_164228-rnmuzwzb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mlp
wandb: ‚≠êÔ∏è View project at https://wandb.ai/kerex-northwestern-university/SB28_7params_3
wandb: üöÄ View run at https://wandb.ai/kerex-northwestern-university/SB28_7params_3/runs/rnmuzwzb
[2025-12-31T16:45:19.109] error: *** JOB 15065005 ON gpua053 CANCELLED AT 2025-12-31T16:45:19 DUE to SIGNAL Terminated ***
