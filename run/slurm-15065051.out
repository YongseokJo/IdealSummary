Python 3.12.1
/u/gkerex/pyenv/torch/bin/python
Job starting on gpua055.delta.ncsa.illinois.edu
Using device: cuda
Loading data from data/camels_LH.hdf5...
Split sizes -> train: 800, val: 100, test: 100
Input dim: 1, Target dim: 6
Parameters: ['Omega_m', 'sigma_8', 'A_SN1', 'A_AGN1', 'A_SN2', 'A_AGN2']
Input stats: mean=2.6878628730773926, std=6.628475666046143
Output stats: min=[0.1002], max=[3.99446]

Operator scope: simple

Initial model:
============================================================
Set-DSR Model Summary
============================================================
Number of summaries (K): 12
Total complexity: 89.00
------------------------------------------------------------
s_0: (identity_s(SUM((m * logM))) + SUM(abs((square(identity(logM)) * (logM - (logM - logM))))))
     complexity: 9.00, depth: 7
s_1: WSUM((logM * (m * m)))
     complexity: 4.00, depth: 4
s_2: square_s((0.6451 * identity_s(identity_s(WMEAN((logM - m))))))
     complexity: 5.50, depth: 7
s_3: WSUM(((((square(logM) + (logM + logM)) * (logM - (m * logM))) * ((square(logM) * m) - ((m - logM) - (m + logM)))) * (logM + (square(1.9845) - abs((logM + m))))))
     complexity: 21.50, depth: 7
s_4: square_s(WMEAN(logM))
     complexity: 3.00, depth: 3
s_5: square_s(identity_s((square_s(identity_s(MEAN(logM))) + (1.7164 - square_s(MEAN(logM))))))
     complexity: 7.50, depth: 7
s_6: -0.0560
     complexity: 0.50, depth: 1
s_7: 1.0424
     complexity: 0.50, depth: 1
s_8: identity_s((WMEAN(((m * 0.6867) * 1.5985)) * (WSUM(logM) + (0.5244 + SUM((logM + m))))))
     complexity: 12.50, depth: 7
s_9: ((square_s(WMEAN((m + -0.3556))) * identity_s((-0.3895 * SUM(logM)))) + ((((0.6086 * (MEAN(logM) + WMEAN(logM))) + identity_s(SUM(m))) - WMEAN(m)) + WSUM(logM)))
     complexity: 22.50, depth: 8
s_10: (-0.7456 * -0.0928)
     complexity: 2.00, depth: 2
s_11: identity_s(0.0313)
     complexity: 0.50, depth: 2
============================================================

Using advanced GP evolver (subtree_mutation=True, const_opt=True)

============================================================
Phase 1: Evolutionary Search (GP)
============================================================

[Gen 0] Advancing curriculum to level 1
[Gen    0] fitness=0.3070, val_mse=8.0030e-02, val_r2=-1.2619, complexity=45.50
[Gen   10] fitness=0.2022, val_mse=6.4020e-02, val_r2=-0.8094, complexity=28.50
[Gen   20] fitness=0.1691, val_mse=6.9928e-02, val_r2=-0.9764, complexity=20.00

[Gen 30] Advancing curriculum to level 2
[Gen   30] fitness=0.1497, val_mse=5.4285e-02, val_r2=-0.5342, complexity=19.00
[Gen   40] fitness=0.1439, val_mse=5.4081e-02, val_r2=-0.5285, complexity=18.00
[Gen   50] fitness=0.1421, val_mse=5.0839e-02, val_r2=-0.4369, complexity=18.00

[Gen 60] Advancing curriculum to level 3
[Gen   60] fitness=0.1313, val_mse=5.3096e-02, val_r2=-0.5007, complexity=15.00
[Gen   70] fitness=0.1224, val_mse=5.8893e-02, val_r2=-0.6645, complexity=15.00
[Gen   80] fitness=0.1221, val_mse=5.8530e-02, val_r2=-0.6543, complexity=15.00
[Gen   90] fitness=0.1218, val_mse=5.8185e-02, val_r2=-0.6445, complexity=15.00
[Gen  100] fitness=0.1215, val_mse=5.7859e-02, val_r2=-0.6353, complexity=15.00
[Gen  110] fitness=0.1213, val_mse=5.7543e-02, val_r2=-0.6264, complexity=15.00
[Gen  120] fitness=0.1110, val_mse=5.5822e-02, val_r2=-0.5777, complexity=13.50
[Gen  130] fitness=0.1102, val_mse=5.5297e-02, val_r2=-0.5629, complexity=13.50
[Gen  140] fitness=0.1097, val_mse=5.4886e-02, val_r2=-0.5513, complexity=13.50
[Gen  150] fitness=0.1092, val_mse=5.4578e-02, val_r2=-0.5425, complexity=13.50
[Gen  160] fitness=0.1089, val_mse=5.4380e-02, val_r2=-0.5369, complexity=13.50
[Gen  170] fitness=0.1087, val_mse=5.4185e-02, val_r2=-0.5314, complexity=13.50
[Gen  180] fitness=0.1085, val_mse=5.4000e-02, val_r2=-0.5262, complexity=13.50
[Gen  190] fitness=0.1084, val_mse=5.3835e-02, val_r2=-0.5215, complexity=13.50
[Gen  200] fitness=0.1083, val_mse=5.3679e-02, val_r2=-0.5171, complexity=13.50
[Gen  210] fitness=0.1082, val_mse=5.3526e-02, val_r2=-0.5128, complexity=13.50
[Gen  220] fitness=0.1081, val_mse=5.3378e-02, val_r2=-0.5086, complexity=13.50
[Gen  230] fitness=0.1080, val_mse=5.3233e-02, val_r2=-0.5045, complexity=13.50
[Gen  240] fitness=0.1075, val_mse=4.4685e-02, val_r2=-0.2629, complexity=13.50
[Gen  250] fitness=0.1071, val_mse=4.4398e-02, val_r2=-0.2548, complexity=13.50
[Gen  260] fitness=0.1069, val_mse=4.4158e-02, val_r2=-0.2480, complexity=13.50
[Gen  270] fitness=0.1066, val_mse=4.1886e-02, val_r2=-0.1838, complexity=14.00
[Gen  280] fitness=0.1064, val_mse=4.1879e-02, val_r2=-0.1836, complexity=14.00
[Gen  290] fitness=0.1063, val_mse=4.1824e-02, val_r2=-0.1821, complexity=14.00

Best fitness: 0.1061

============================================================
Phase 2b: Joint Training (MLP + Learnable Program Constants)
============================================================
Number of learnable constants in programs: 10
[Epoch    0] train_loss=2.9116e-02, val_loss=2.7017e-02, val_r2=0.2364
[Epoch   10] train_loss=2.1054e-02, val_loss=2.2139e-02, val_r2=0.3743
[Epoch   20] train_loss=2.0261e-02, val_loss=2.1661e-02, val_r2=0.3878
[Epoch   30] train_loss=1.9764e-02, val_loss=2.1433e-02, val_r2=0.3942
[Epoch   40] train_loss=1.9952e-02, val_loss=2.1661e-02, val_r2=0.3878
[Epoch   50] train_loss=1.9651e-02, val_loss=2.1735e-02, val_r2=0.3857
[Epoch   60] train_loss=1.9587e-02, val_loss=2.1538e-02, val_r2=0.3913
[Epoch   70] train_loss=1.9571e-02, val_loss=2.1517e-02, val_r2=0.3919
[Epoch   80] train_loss=1.9556e-02, val_loss=2.1512e-02, val_r2=0.3920
[Epoch   90] train_loss=1.9541e-02, val_loss=2.1543e-02, val_r2=0.3911
[Epoch  100] train_loss=1.9535e-02, val_loss=2.1534e-02, val_r2=0.3914
[Epoch  110] train_loss=1.9536e-02, val_loss=2.1517e-02, val_r2=0.3918
[Epoch  120] train_loss=1.9533e-02, val_loss=2.1531e-02, val_r2=0.3915
[Epoch  130] train_loss=1.9531e-02, val_loss=2.1525e-02, val_r2=0.3917
[Epoch  140] train_loss=1.9531e-02, val_loss=2.1526e-02, val_r2=0.3916
[Epoch  150] train_loss=1.9531e-02, val_loss=2.1527e-02, val_r2=0.3916
[Epoch  160] train_loss=1.9531e-02, val_loss=2.1527e-02, val_r2=0.3916
[Epoch  170] train_loss=1.9531e-02, val_loss=2.1527e-02, val_r2=0.3916
[Epoch  180] train_loss=1.9531e-02, val_loss=2.1526e-02, val_r2=0.3916
[Epoch  190] train_loss=1.9531e-02, val_loss=2.1526e-02, val_r2=0.3916

Best validation loss: 2.1331e-02
Learned 10 program constants via backprop

Training completed in 8349.9s

Final model:
============================================================
Set-DSR Model Summary
============================================================
Number of summaries (K): 12
Total complexity: 14.00
------------------------------------------------------------
s_0: WSUM(logM)
     complexity: 2.00, depth: 2
s_1: MEAN(-0.4724)
     complexity: 1.50, depth: 2
s_2: identity_s(identity_s(-0.3129))
     complexity: 0.50, depth: 3
s_3: identity_s(-2.0088)
     complexity: 0.50, depth: 2
s_4: MEAN(-1.4709)
     complexity: 1.50, depth: 2
s_5: MEAN(-1.0449)
     complexity: 1.50, depth: 2
s_6: MEAN(logM)
     complexity: 1.00, depth: 2
s_7: -1.3759
     complexity: 0.50, depth: 1
s_8: 0.7407
     complexity: 0.50, depth: 1
s_9: -1.8043
     complexity: 0.50, depth: 1
s_10: square_s(1.4595)
     complexity: 1.50, depth: 2
s_11: WSUM(1.7193)
     complexity: 2.50, depth: 2
============================================================

Final Train MSE: 2.9904e-01, R2: 0.4528
Final Val MSE: 3.2349e-01, R2: 0.3971
Final Test MSE: 3.2408e-01, R2: 0.4044
Saved test plot: run/data/models/plots/set_dsr/pred_vs_true_test_final.png
Saved plots: run/data/models/plots/set_dsr/pred_vs_true_train_final.png, run/data/models/plots/set_dsr/pred_vs_true_val_final.png

Applying SymPy simplification...
Warning: SymPy simplification failed: 'SetDSR' object has no attribute 'items'
Saved expressions: run/data/models/set_dsr/expressions_20251231_193428.json
Saved checkpoint: run/data/models/set_dsr/set_dsr_20251231_193428.pt
Saved history: run/data/models/set_dsr/history_20251231_193428.json

============================================================
Learned Symbolic Summary Statistics:
============================================================
  s_0: WSUM(logM)
  s_1: MEAN(-0.4724)
  s_2: identity_s(identity_s(-0.3129))
  s_3: identity_s(-2.0088)
  s_4: MEAN(-1.4709)
  s_5: MEAN(-1.0449)
  s_6: MEAN(logM)
  s_7: -1.3759
  s_8: 0.7407
  s_9: -1.8043
  s_10: square_s(1.4595)
  s_11: WSUM(1.7193)
============================================================
Job completed successfully
