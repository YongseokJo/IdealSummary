
Initial model:
============================================================
Set-DSR Model Summary
============================================================
Number of summaries (K): 12
Total complexity: 89.00
------------------------------------------------------------
s_0: (identity_s(SUM((m * logM))) + SUM(abs((square(identity(logM)) * (logM - (logM - logM))))))
     complexity: 9.00, depth: 7
s_1: WSUM((logM * (m * m)))
     complexity: 4.00, depth: 4
s_2: square_s((0.6451 * identity_s(identity_s(WMEAN((logM - m))))))
     complexity: 5.50, depth: 7
s_3: WSUM(((((square(logM) + (logM + logM)) * (logM - (m * logM))) * ((square(logM) * m) - ((m - logM) - (m + logM)))) * (logM + (square(1.9845) - abs((logM + m))))))
     complexity: 21.50, depth: 7
s_4: square_s(WMEAN(logM))
     complexity: 3.00, depth: 3
s_5: square_s(identity_s((square_s(identity_s(MEAN(logM))) + (1.7164 - square_s(MEAN(logM))))))
     complexity: 7.50, depth: 7
s_6: -0.0560
     complexity: 0.50, depth: 1
s_7: 1.0424
     complexity: 0.50, depth: 1
s_8: identity_s((WMEAN(((m * 0.6867) * 1.5985)) * (WSUM(logM) + (0.5244 + SUM((logM + m))))))
     complexity: 12.50, depth: 7
s_9: ((square_s(WMEAN((m + -0.3556))) * identity_s((-0.3895 * SUM(logM)))) + ((((0.6086 * (MEAN(logM) + WMEAN(logM))) + identity_s(SUM(m))) - WMEAN(m)) + WSUM(logM)))
     complexity: 22.50, depth: 8
s_10: (-0.7456 * -0.0928)
     complexity: 2.00, depth: 2
s_11: identity_s(0.0313)
     complexity: 0.50, depth: 2
============================================================

Using advanced GP evolver (subtree_mutation=True, const_opt=True)

============================================================
Phase 1: Evolutionary Search (GP)
============================================================

[Gen 0] Advancing curriculum to level 1
[Gen    0] fitness=0.3070, val_mse=8.0030e-02, val_r2=-1.2619, complexity=45.50
[Gen   10] fitness=0.2022, val_mse=6.4020e-02, val_r2=-0.8094, complexity=28.50
[Gen   20] fitness=0.1691, val_mse=6.9928e-02, val_r2=-0.9764, complexity=20.00

[Gen 30] Advancing curriculum to level 2
[Gen   30] fitness=0.1497, val_mse=5.4285e-02, val_r2=-0.5342, complexity=19.00
[Gen   40] fitness=0.1457, val_mse=5.6890e-02, val_r2=-0.6079, complexity=18.00
[Gen   50] fitness=0.1452, val_mse=5.6358e-02, val_r2=-0.5929, complexity=18.00

[Gen 60] Advancing curriculum to level 3
[Gen   60] fitness=0.1431, val_mse=5.3805e-02, val_r2=-0.5207, complexity=18.00
[Gen   70] fitness=0.1430, val_mse=5.3661e-02, val_r2=-0.5166, complexity=18.00
[Gen   80] fitness=0.1344, val_mse=5.5487e-02, val_r2=-0.5682, complexity=16.00
[Gen   90] fitness=0.1311, val_mse=4.7931e-02, val_r2=-0.3547, complexity=16.50
[Gen  100] fitness=0.1258, val_mse=4.3710e-02, val_r2=-0.2354, complexity=16.50
[Gen  110] fitness=0.1240, val_mse=4.2036e-02, val_r2=-0.1881, complexity=16.50
[Gen  120] fitness=0.1231, val_mse=4.1085e-02, val_r2=-0.1612, complexity=16.50
[Gen  130] fitness=0.1220, val_mse=3.9942e-02, val_r2=-0.1289, complexity=16.50
[Gen  140] fitness=0.1204, val_mse=3.8319e-02, val_r2=-0.0830, complexity=16.50
[Gen  150] fitness=0.1150, val_mse=3.7886e-02, val_r2=-0.0708, complexity=15.50
[Gen  160] fitness=0.1143, val_mse=3.7086e-02, val_r2=-0.0482, complexity=15.50
[Gen  170] fitness=0.1126, val_mse=3.2640e-02, val_r2=0.0775, complexity=16.00
[Gen  180] fitness=0.1120, val_mse=3.2014e-02, val_r2=0.0952, complexity=16.00
[Gen  190] fitness=0.1116, val_mse=3.1541e-02, val_r2=0.1086, complexity=16.00
[Gen  200] fitness=0.1112, val_mse=3.1158e-02, val_r2=0.1194, complexity=16.00
[Gen  210] fitness=0.1108, val_mse=3.0802e-02, val_r2=0.1295, complexity=16.00
[Gen  220] fitness=0.1105, val_mse=3.0524e-02, val_r2=0.1373, complexity=16.00
[Gen  230] fitness=0.1104, val_mse=3.0328e-02, val_r2=0.1428, complexity=16.00
[Gen  240] fitness=0.1102, val_mse=3.0165e-02, val_r2=0.1474, complexity=16.00
[Gen  250] fitness=0.1101, val_mse=3.0018e-02, val_r2=0.1516, complexity=16.00
[Gen  260] fitness=0.1099, val_mse=2.9886e-02, val_r2=0.1553, complexity=16.00
[Gen  270] fitness=0.1098, val_mse=2.9766e-02, val_r2=0.1587, complexity=16.00
[Gen  280] fitness=0.1097, val_mse=2.9656e-02, val_r2=0.1618, complexity=16.00
[Gen  290] fitness=0.1096, val_mse=2.9559e-02, val_r2=0.1646, complexity=16.00

Best fitness: 0.1095

============================================================
Phase 2b: Joint Training (MLP + Learnable Program Constants)
============================================================
Number of learnable constants in programs: 6
[Epoch    0] train_loss=2.7943e-02, val_loss=2.6338e-02, val_r2=0.2556
[Epoch   10] train_loss=2.1093e-02, val_loss=2.2010e-02, val_r2=0.3779
[Epoch   20] train_loss=2.0202e-02, val_loss=2.1460e-02, val_r2=0.3935
[Epoch   30] train_loss=1.9587e-02, val_loss=2.1336e-02, val_r2=0.3970
[Epoch   40] train_loss=1.9758e-02, val_loss=2.1415e-02, val_r2=0.3947
[Epoch   50] train_loss=1.9165e-02, val_loss=2.1129e-02, val_r2=0.4028
[Epoch   60] train_loss=1.8990e-02, val_loss=2.1017e-02, val_r2=0.4060
[Epoch   70] train_loss=1.8984e-02, val_loss=2.0982e-02, val_r2=0.4070
[Epoch   80] train_loss=1.8953e-02, val_loss=2.1084e-02, val_r2=0.4041
[Epoch   90] train_loss=1.8938e-02, val_loss=2.1108e-02, val_r2=0.4034
[Epoch  100] train_loss=1.8920e-02, val_loss=2.1080e-02, val_r2=0.4042
[Epoch  110] train_loss=1.8917e-02, val_loss=2.1054e-02, val_r2=0.4050
[Epoch  120] train_loss=1.8913e-02, val_loss=2.1080e-02, val_r2=0.4042
[Epoch  130] train_loss=1.8908e-02, val_loss=2.1066e-02, val_r2=0.4046
[Epoch  140] train_loss=1.8907e-02, val_loss=2.1067e-02, val_r2=0.4046
[Epoch  150] train_loss=1.8907e-02, val_loss=2.1066e-02, val_r2=0.4046
[Epoch  160] train_loss=1.8906e-02, val_loss=2.1066e-02, val_r2=0.4046
[Epoch  170] train_loss=1.8906e-02, val_loss=2.1066e-02, val_r2=0.4046
[Epoch  180] train_loss=1.8906e-02, val_loss=2.1066e-02, val_r2=0.4046
[Epoch  190] train_loss=1.8906e-02, val_loss=2.1066e-02, val_r2=0.4046

Best validation loss: 2.0917e-02
Learned 6 program constants via backprop

Training completed in 4443.8s

Final model:
============================================================
Set-DSR Model Summary
============================================================
Number of summaries (K): 12
Total complexity: 16.00
------------------------------------------------------------
s_0: WSUM(m)
     complexity: 2.00, depth: 2
s_1: MEAN(logM)
     complexity: 1.00, depth: 2
s_2: identity_s(SUM((m - identity(m))))
     complexity: 2.00, depth: 5
s_3: identity_s(-1.0999)
     complexity: 0.50, depth: 2
s_4: MEAN(-1.6648)
     complexity: 1.50, depth: 2
s_5: MEAN(0.4070)
     complexity: 1.50, depth: 2
s_6: MEAN(m)
     complexity: 1.00, depth: 2
s_7: -1.1044
     complexity: 0.50, depth: 1
s_8: MAX(logM)
     complexity: 2.00, depth: 2
s_9: -1.0676
     complexity: 0.50, depth: 1
s_10: square_s(identity_s(identity_s(-2.2168)))
     complexity: 1.50, depth: 4
s_11: WMEAN(m)
     complexity: 2.00, depth: 2
============================================================

Final Train MSE: 2.9781e-01, R2: 0.4551
Final Val MSE: 3.1721e-01, R2: 0.4088
Final Test MSE: 3.2406e-01, R2: 0.4044
Saved test plot: run/data/models/plots/set_dsr/pred_vs_true_test_final.png
Saved plots: run/data/models/plots/set_dsr/pred_vs_true_train_final.png, run/data/models/plots/set_dsr/pred_vs_true_val_final.png

Applying SymPy simplification...
Warning: SymPy simplification failed: 'SetDSR' object has no attribute 'items'
Saved expressions: run/data/models/set_dsr/expressions_20251231_181913.json
Saved checkpoint: run/data/models/set_dsr/set_dsr_20251231_181913.pt
Saved history: run/data/models/set_dsr/history_20251231_181913.json

============================================================
Learned Symbolic Summary Statistics:
============================================================
  s_0: WSUM(m)
  s_1: MEAN(logM)
  s_2: identity_s(SUM((m - identity(m))))
  s_3: identity_s(-1.0999)
  s_4: MEAN(-1.6648)
  s_5: MEAN(0.4070)
  s_6: MEAN(m)
  s_7: -1.1044
  s_8: MAX(logM)
  s_9: -1.0676
  s_10: square_s(identity_s(identity_s(-2.2168)))
  s_11: WMEAN(m)
============================================================
